{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_85</th>\n",
       "      <th>feat_86</th>\n",
       "      <th>feat_87</th>\n",
       "      <th>feat_88</th>\n",
       "      <th>feat_89</th>\n",
       "      <th>feat_90</th>\n",
       "      <th>feat_91</th>\n",
       "      <th>feat_92</th>\n",
       "      <th>feat_93</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  feat_9  \\\n",
       "0   1       1       0       0       0       0       0       0       0       0   \n",
       "1   2       0       0       0       0       0       0       0       1       0   \n",
       "2   3       0       0       0       0       0       0       0       1       0   \n",
       "3   4       1       0       0       1       6       1       5       0       0   \n",
       "4   5       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   ...  feat_85  feat_86  feat_87  feat_88  feat_89  feat_90  feat_91  \\\n",
       "0  ...        1        0        0        0        0        0        0   \n",
       "1  ...        0        0        0        0        0        0        0   \n",
       "2  ...        0        0        0        0        0        0        0   \n",
       "3  ...        0        1        2        0        0        0        0   \n",
       "4  ...        1        0        0        0        0        1        0   \n",
       "\n",
       "   feat_92  feat_93   target  \n",
       "0        0        0  Class_1  \n",
       "1        0        0  Class_1  \n",
       "2        0        0  Class_1  \n",
       "3        0        0  Class_1  \n",
       "4        0        0  Class_1  \n",
       "\n",
       "[5 rows x 95 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "data = pd.read_csv(\"../3. 실습데이터/otto_train.csv\") # Product Category\n",
    "data.head() # 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nid: 고유 아이디\\nfeat_1 ~ feat_93: 설명변수\\ntarget: 타겟변수 (1~9)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "id: 고유 아이디\n",
    "feat_1 ~ feat_93: 설명변수\n",
    "target: 타겟변수 (1~9)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nCar: 61878 nVar: 95\n"
     ]
    }
   ],
   "source": [
    "nCar = data.shape[0] # 데이터 개수\n",
    "nVar = data.shape[1] # 변수 개수\n",
    "print('nCar: %d' % nCar, 'nVar: %d' % nVar )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 의미가 없다고 판단되는 변수 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['id'], axis = 1) # id 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 타겟 변수의 문자열을 숫자로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_dict = {\"Class_1\": 1,\n",
    "                \"Class_2\": 2,\n",
    "                \"Class_3\": 3,\n",
    "                \"Class_4\": 4,\n",
    "                \"Class_5\": 5,\n",
    "                \"Class_6\": 6,\n",
    "                \"Class_7\": 7,\n",
    "                \"Class_8\": 8,\n",
    "                \"Class_9\": 9}\n",
    "after_mapping_target = data['target'].apply(lambda x: mapping_dict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 설명변수와 타겟변수를 분리, 학습데이터와 평가데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49502, 93) (12376, 93) (49502,) (12376,)\n"
     ]
    }
   ],
   "source": [
    "feature_columns = list(data.columns.difference(['target'])) # target을 제외한 모든 행\n",
    "X = data[feature_columns] # 설명변수\n",
    "y = after_mapping_target # 타겟변수\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size = 0.2, random_state = 42) # 학습데이터와 평가데이터의 비율을 8:2 로 분할| \n",
    "print(train_x.shape, test_x.shape, train_y.shape, test_y.shape) # 데이터 개수 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:21:47] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "Accuracy: 76.67 %\n",
      "Time: 6.91 seconds\n"
     ]
    }
   ],
   "source": [
    "# !pip install xgboost\n",
    "import xgboost as xgb\n",
    "import time\n",
    "start = time.time() # 시작 시간 지정\n",
    "xgb_dtrain = xgb.DMatrix(data = train_x, label = train_y) # 학습 데이터를 XGBoost 모델에 맞게 변환\n",
    "xgb_dtest = xgb.DMatrix(data = test_x) # 평가 데이터를 XGBoost 모델에 맞게 변환\n",
    "xgb_param = {'max_depth': 10, # 트리 깊이\n",
    "         'learning_rate': 0.01, # Step Size\n",
    "         'n_estimators': 100, # Number of trees, 트리 생성 개수\n",
    "         'objective': 'multi:softmax', # 목적 함수\n",
    "        'num_class': len(set(train_y)) + 1} # 파라미터 추가, Label must be in [0, num_class) -> num_class보다 1 커야한다.\n",
    "xgb_model = xgb.train(params = xgb_param, dtrain = xgb_dtrain) # 학습 진행\n",
    "xgb_model_predict = xgb_model.predict(xgb_dtest) # 평가 데이터 예측\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(test_y, xgb_model_predict) * 100), \"%\") # 정확도 % 계산\n",
    "print(\"Time: %.2f\" % (time.time() - start), \"seconds\") # 코드 실행 시간 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 3., 6., ..., 9., 2., 7.], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Younjiei\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019087 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3110\n",
      "[LightGBM] [Info] Number of data points in the train set: 49502, number of used features: 93\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -3.476745\n",
      "[LightGBM] [Info] Start training from score -1.341381\n",
      "[LightGBM] [Info] Start training from score -2.039019\n",
      "[LightGBM] [Info] Start training from score -3.135151\n",
      "[LightGBM] [Info] Start training from score -3.125444\n",
      "[LightGBM] [Info] Start training from score -1.481556\n",
      "[LightGBM] [Info] Start training from score -3.074772\n",
      "[LightGBM] [Info] Start training from score -1.986562\n",
      "[LightGBM] [Info] Start training from score -2.533374\n",
      "Accuracy: 76.28 %\n",
      "Time: 4.95 seconds\n"
     ]
    }
   ],
   "source": [
    "# !pip install lightgbm\n",
    "import lightgbm as lgb\n",
    "start = time.time() # 시작 시간 지정\n",
    "lgb_dtrain = lgb.Dataset(data = train_x, label = train_y) # 학습 데이터를 LightGBM 모델에 맞게 변환\n",
    "lgb_param = {'max_depth': 10, # 트리 깊이\n",
    "            'learning_rate': 0.01, # Step Size\n",
    "            'n_estimators': 100, # Number of trees, 트리 생성 개수\n",
    "            'objective': 'multiclass', # 목적 함수\n",
    "            'num_class': len(set(train_y)) + 1} # 파라미터 추가, Label must be in [0, num_class) -> num_class보다 1 커야한다.\n",
    "lgb_model = lgb.train(params = lgb_param, train_set = lgb_dtrain) # 학습 진행\n",
    "lgb_model_predict = np.argmax(lgb_model.predict(test_x), axis = 1) # 평가 데이터 예측, Softmax의 결과값 중 가장 큰 값의 Label로 예측\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(test_y, lgb_model_predict) * 100), \"%\") # 정확도 % 계산\n",
    "print(\"Time: %.2f\" % (time.time() - start), \"seconds\") # 코드 실행 시간 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.01734061e-15, 2.25081693e-02, 3.62193933e-01, ...,\n",
       "        3.24234521e-02, 5.82126692e-02, 3.67722414e-02],\n",
       "       [1.14084116e-15, 5.36978636e-02, 1.90687128e-01, ...,\n",
       "        3.25081119e-01, 9.38028846e-02, 6.50463131e-02],\n",
       "       [5.94595781e-16, 9.66842220e-03, 5.82817482e-02, ...,\n",
       "        1.42318289e-02, 3.40230275e-02, 2.14919364e-02],\n",
       "       ...,\n",
       "       [7.09105769e-16, 4.63740004e-02, 1.08297559e-01, ...,\n",
       "        5.46934960e-02, 7.24513712e-02, 5.74635996e-01],\n",
       "       [9.88127136e-16, 1.54895684e-02, 5.45515599e-01, ...,\n",
       "        2.45870954e-02, 5.65410617e-02, 3.62344513e-02],\n",
       "       [7.59617500e-16, 1.49480877e-02, 7.44570300e-02, ...,\n",
       "        5.76695793e-01, 1.43227106e-01, 2.74567219e-02]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_model.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5907034\ttotal: 808ms\tremaining: 1m 20s\n",
      "1:\tlearn: 0.6356107\ttotal: 1.59s\tremaining: 1m 18s\n",
      "2:\tlearn: 0.6411256\ttotal: 2.3s\tremaining: 1m 14s\n",
      "3:\tlearn: 0.6480344\ttotal: 3.05s\tremaining: 1m 13s\n",
      "4:\tlearn: 0.6508222\ttotal: 3.72s\tremaining: 1m 10s\n",
      "5:\tlearn: 0.6499939\ttotal: 4.44s\tremaining: 1m 9s\n",
      "6:\tlearn: 0.6507818\ttotal: 5.21s\tremaining: 1m 9s\n",
      "7:\tlearn: 0.6548422\ttotal: 5.92s\tremaining: 1m 8s\n",
      "8:\tlearn: 0.6559533\ttotal: 6.6s\tremaining: 1m 6s\n",
      "9:\tlearn: 0.6560947\ttotal: 7.38s\tremaining: 1m 6s\n",
      "10:\tlearn: 0.6568421\ttotal: 8.14s\tremaining: 1m 5s\n",
      "11:\tlearn: 0.6588219\ttotal: 8.91s\tremaining: 1m 5s\n",
      "12:\tlearn: 0.6592259\ttotal: 9.63s\tremaining: 1m 4s\n",
      "13:\tlearn: 0.6611248\ttotal: 10.4s\tremaining: 1m 3s\n",
      "14:\tlearn: 0.6625591\ttotal: 11.2s\tremaining: 1m 3s\n",
      "15:\tlearn: 0.6631853\ttotal: 11.8s\tremaining: 1m 2s\n",
      "16:\tlearn: 0.6639328\ttotal: 12.5s\tremaining: 1m 1s\n",
      "17:\tlearn: 0.6668821\ttotal: 13.2s\tremaining: 1m\n",
      "18:\tlearn: 0.6669630\ttotal: 13.9s\tremaining: 59.1s\n",
      "19:\tlearn: 0.6675286\ttotal: 14.5s\tremaining: 58s\n",
      "20:\tlearn: 0.6673266\ttotal: 15.1s\tremaining: 56.9s\n",
      "21:\tlearn: 0.6677104\ttotal: 15.9s\tremaining: 56.3s\n",
      "22:\tlearn: 0.6682558\ttotal: 16.6s\tremaining: 55.6s\n",
      "23:\tlearn: 0.6683972\ttotal: 17.5s\tremaining: 55.5s\n",
      "24:\tlearn: 0.6686599\ttotal: 18.3s\tremaining: 55s\n",
      "25:\tlearn: 0.6681952\ttotal: 19.2s\tremaining: 54.5s\n",
      "26:\tlearn: 0.6684982\ttotal: 20.1s\tremaining: 54.2s\n",
      "27:\tlearn: 0.6692053\ttotal: 20.9s\tremaining: 53.7s\n",
      "28:\tlearn: 0.6696699\ttotal: 21.6s\tremaining: 53s\n",
      "29:\tlearn: 0.6699325\ttotal: 22.4s\tremaining: 52.2s\n",
      "30:\tlearn: 0.6705992\ttotal: 23.1s\tremaining: 51.3s\n",
      "31:\tlearn: 0.6709426\ttotal: 23.8s\tremaining: 50.5s\n",
      "32:\tlearn: 0.6708012\ttotal: 24.5s\tremaining: 49.7s\n",
      "33:\tlearn: 0.6709426\ttotal: 25.2s\tremaining: 48.9s\n",
      "34:\tlearn: 0.6707002\ttotal: 25.9s\tremaining: 48.1s\n",
      "35:\tlearn: 0.6715082\ttotal: 26.6s\tremaining: 47.3s\n",
      "36:\tlearn: 0.6705992\ttotal: 27.3s\tremaining: 46.4s\n",
      "37:\tlearn: 0.6725991\ttotal: 27.9s\tremaining: 45.6s\n",
      "38:\tlearn: 0.6729829\ttotal: 28.6s\tremaining: 44.7s\n",
      "39:\tlearn: 0.6725991\ttotal: 29.3s\tremaining: 43.9s\n",
      "40:\tlearn: 0.6734273\ttotal: 30s\tremaining: 43.1s\n",
      "41:\tlearn: 0.6738314\ttotal: 30.7s\tremaining: 42.4s\n",
      "42:\tlearn: 0.6741546\ttotal: 31.4s\tremaining: 41.7s\n",
      "43:\tlearn: 0.6739728\ttotal: 32.1s\tremaining: 40.9s\n",
      "44:\tlearn: 0.6741950\ttotal: 32.8s\tremaining: 40s\n",
      "45:\tlearn: 0.6750636\ttotal: 33.4s\tremaining: 39.2s\n",
      "46:\tlearn: 0.6758919\ttotal: 34.1s\tremaining: 38.4s\n",
      "47:\tlearn: 0.6757707\ttotal: 34.8s\tremaining: 37.7s\n",
      "48:\tlearn: 0.6762151\ttotal: 35.5s\tremaining: 36.9s\n",
      "49:\tlearn: 0.6774474\ttotal: 36.2s\tremaining: 36.2s\n",
      "50:\tlearn: 0.6777100\ttotal: 36.8s\tremaining: 35.4s\n",
      "51:\tlearn: 0.6786594\ttotal: 37.6s\tremaining: 34.7s\n",
      "52:\tlearn: 0.6789827\ttotal: 38.4s\tremaining: 34s\n",
      "53:\tlearn: 0.6804372\ttotal: 39.1s\tremaining: 33.3s\n",
      "54:\tlearn: 0.6804372\ttotal: 39.8s\tremaining: 32.6s\n",
      "55:\tlearn: 0.6809220\ttotal: 40.5s\tremaining: 31.9s\n",
      "56:\tlearn: 0.6812250\ttotal: 41.2s\tremaining: 31.1s\n",
      "57:\tlearn: 0.6813058\ttotal: 42s\tremaining: 30.4s\n",
      "58:\tlearn: 0.6811846\ttotal: 42.7s\tremaining: 29.7s\n",
      "59:\tlearn: 0.6813260\ttotal: 43.4s\tremaining: 28.9s\n",
      "60:\tlearn: 0.6816694\ttotal: 44.1s\tremaining: 28.2s\n",
      "61:\tlearn: 0.6823159\ttotal: 44.9s\tremaining: 27.5s\n",
      "62:\tlearn: 0.6832653\ttotal: 45.6s\tremaining: 26.8s\n",
      "63:\tlearn: 0.6840734\ttotal: 46.3s\tremaining: 26s\n",
      "64:\tlearn: 0.6840734\ttotal: 47s\tremaining: 25.3s\n",
      "65:\tlearn: 0.6846592\ttotal: 47.6s\tremaining: 24.5s\n",
      "66:\tlearn: 0.6843360\ttotal: 48.4s\tremaining: 23.8s\n",
      "67:\tlearn: 0.6846390\ttotal: 49s\tremaining: 23.1s\n",
      "68:\tlearn: 0.6854269\ttotal: 49.7s\tremaining: 22.3s\n",
      "69:\tlearn: 0.6858309\ttotal: 50.4s\tremaining: 21.6s\n",
      "70:\tlearn: 0.6858309\ttotal: 51.2s\tremaining: 20.9s\n",
      "71:\tlearn: 0.6865783\ttotal: 51.9s\tremaining: 20.2s\n",
      "72:\tlearn: 0.6864167\ttotal: 52.6s\tremaining: 19.5s\n",
      "73:\tlearn: 0.6868611\ttotal: 53.3s\tremaining: 18.7s\n",
      "74:\tlearn: 0.6869217\ttotal: 54s\tremaining: 18s\n",
      "75:\tlearn: 0.6870429\ttotal: 54.7s\tremaining: 17.3s\n",
      "76:\tlearn: 0.6875278\ttotal: 55.4s\tremaining: 16.6s\n",
      "77:\tlearn: 0.6881136\ttotal: 56.1s\tremaining: 15.8s\n",
      "78:\tlearn: 0.6883762\ttotal: 57s\tremaining: 15.1s\n",
      "79:\tlearn: 0.6888207\ttotal: 57.7s\tremaining: 14.4s\n",
      "80:\tlearn: 0.6892449\ttotal: 58.3s\tremaining: 13.7s\n",
      "81:\tlearn: 0.6898509\ttotal: 59s\tremaining: 12.9s\n",
      "82:\tlearn: 0.6897095\ttotal: 59.6s\tremaining: 12.2s\n",
      "83:\tlearn: 0.6902549\ttotal: 1m\tremaining: 11.5s\n",
      "84:\tlearn: 0.6909822\ttotal: 1m\tremaining: 10.7s\n",
      "85:\tlearn: 0.6910832\ttotal: 1m 1s\tremaining: 10s\n",
      "86:\tlearn: 0.6914468\ttotal: 1m 2s\tremaining: 9.3s\n",
      "87:\tlearn: 0.6916084\ttotal: 1m 2s\tremaining: 8.57s\n",
      "88:\tlearn: 0.6919922\ttotal: 1m 3s\tremaining: 7.85s\n",
      "89:\tlearn: 0.6925579\ttotal: 1m 4s\tremaining: 7.16s\n",
      "90:\tlearn: 0.6928407\ttotal: 1m 5s\tremaining: 6.45s\n",
      "91:\tlearn: 0.6930427\ttotal: 1m 6s\tremaining: 5.74s\n",
      "92:\tlearn: 0.6935073\ttotal: 1m 6s\tremaining: 5.03s\n",
      "93:\tlearn: 0.6940932\ttotal: 1m 7s\tremaining: 4.32s\n",
      "94:\tlearn: 0.6944972\ttotal: 1m 8s\tremaining: 3.61s\n",
      "95:\tlearn: 0.6948810\ttotal: 1m 9s\tremaining: 2.89s\n",
      "96:\tlearn: 0.6951840\ttotal: 1m 10s\tremaining: 2.17s\n",
      "97:\tlearn: 0.6954264\ttotal: 1m 10s\tremaining: 1.45s\n",
      "98:\tlearn: 0.6955881\ttotal: 1m 11s\tremaining: 724ms\n",
      "99:\tlearn: 0.6956285\ttotal: 1m 12s\tremaining: 0us\n",
      "Accuracy: 69.64 %\n",
      "Time: 72.58 seconds\n"
     ]
    }
   ],
   "source": [
    "# !pip install catboost\n",
    "import catboost as cb\n",
    "start = time.time() # 시작 시간 지정\n",
    "cb_dtrain = cb.Pool(data = train_x, label = train_y) # 학습 데이터를 Catboost 모델에 맞게 변환\n",
    "cb_param = {'max_depth': 10, # 트리 깊이\n",
    "            'learning_rate': 0.01, # Step Size\n",
    "            'n_estimators': 100, # Number of trees, 트리 생성 개수\n",
    "            'eval_metric': 'Accuracy', # 평가 척도\n",
    "            'loss_function': 'MultiClass'} # 손실 함수, 목적 함수\n",
    "cb_model = cb.train(pool = cb_dtrain, params = cb_param) # 학습 진행\n",
    "cb_model_predict = np.argmax(cb_model.predict(test_x), axis = 1) + 1 # 평가 데이터 예측, Softmax의 결과값 중 가장 큰 값의 Label로 예측, 인덱스의 순서를 맞추기 위해 +1\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(test_y, cb_model_predict) * 100), \"%\") # 정확도 % 계산\n",
    "print(\"Time: %.2f\" % (time.time() - start), \"seconds\") # 코드 실행 시간 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.35426047,  1.22109587,  0.44230101, ..., -0.1698448 ,\n",
       "        -0.02059177, -0.2130643 ],\n",
       "       [-0.07235138,  0.42535181,  0.20060428, ...,  0.21863604,\n",
       "         0.2719157 ,  0.25089315],\n",
       "       [-0.3315885 , -0.31862353, -0.31279765, ..., -0.29798357,\n",
       "        -0.24018767, -0.32984969],\n",
       "       ...,\n",
       "       [ 0.05304325,  0.02500267, -0.14752573, ..., -0.20741963,\n",
       "         0.12789417,  1.51166757],\n",
       "       [-0.55093666,  1.7691278 ,  0.99746884, ..., -0.3420542 ,\n",
       "        -0.49799871, -0.38136323],\n",
       "       [-0.3033724 ,  0.09352675, -0.11808658, ...,  0.65825036,\n",
       "         1.05515787, -0.20799899]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb_model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id             date     price  bedrooms  bathrooms  floors  \\\n",
       "0  7129300520  20141013T000000  221900.0         3       1.00     1.0   \n",
       "1  6414100192  20141209T000000  538000.0         3       2.25     2.0   \n",
       "2  5631500400  20150225T000000  180000.0         2       1.00     1.0   \n",
       "3  2487200875  20141209T000000  604000.0         4       3.00     1.0   \n",
       "4  1954400510  20150218T000000  510000.0         3       2.00     1.0   \n",
       "\n",
       "   waterfront  condition  grade  yr_built  yr_renovated  zipcode      lat  \\\n",
       "0           0          3      7      1955             0    98178  47.5112   \n",
       "1           0          3      7      1951          1991    98125  47.7210   \n",
       "2           0          3      6      1933             0    98028  47.7379   \n",
       "3           0          5      7      1965             0    98136  47.5208   \n",
       "4           0          3      8      1987             0    98074  47.6168   \n",
       "\n",
       "      long  \n",
       "0 -122.257  \n",
       "1 -122.319  \n",
       "2 -122.233  \n",
       "3 -122.393  \n",
       "4 -122.045  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "data = pd.read_csv(\"../3. 실습데이터/kc_house_data.csv\") \n",
    "data.head() # 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['id', 'date', 'zipcode', 'lat', 'long'], axis = 1) # id, date, zipcode, lat, long  제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15129, 8) (6484, 8) (15129,) (6484,)\n"
     ]
    }
   ],
   "source": [
    "feature_columns = list(data.columns.difference(['price'])) # Price를 제외한 모든 행\n",
    "X = data[feature_columns]\n",
    "y = data['price']\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size = 0.3, random_state = 42) # 학습데이터와 평가데이터의 비율을 7:3\n",
    "print(train_x.shape, test_x.shape, train_y.shape, test_y.shape) # 데이터 개수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Younjiei\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000453 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 237\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 537729.263666\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "# !pip install lightgbm\n",
    "import lightgbm as lgb\n",
    "start = time.time() # 시작 시간 지정\n",
    "lgb_dtrain = lgb.Dataset(data = train_x, label = train_y) # 학습 데이터를 LightGBM 모델에 맞게 변환\n",
    "lgb_param = {'max_depth': 10, # 트리 깊이\n",
    "            'learning_rate': 0.01, # Step Size\n",
    "            'n_estimators': 500, # Number of trees, 트리 생성 개수\n",
    "            'objective': 'regression'} # 파라미터 추가, Label must be in [0, num_class) -> num_class보다 1 커야한다.\n",
    "lgb_model = lgb.train(params = lgb_param, train_set = lgb_dtrain) # 학습 진행\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210904.17249451784"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from math import sqrt\n",
    "\n",
    "sqrt(mean_squared_error(lgb_model.predict(test_x),test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9490\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000416 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 230\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 538647.523762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Younjiei\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9630\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000328 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 234\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 535725.869787\n",
      "9572\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000352 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 235\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 542923.864961\n",
      "9651\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000319 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 233\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 538931.680349\n",
      "9545\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000324 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 232\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 537329.272523\n",
      "9510\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000511 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 231\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 541322.277877\n",
      "9507\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000433 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 233\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 534995.840307\n",
      "9560\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000318 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 229\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 542181.600040\n",
      "9604\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000897 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 232\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 534867.574658\n",
      "9551\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000338 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 235\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 535154.131073\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "bagging_predict_result = [] # 빈 리스트 생성\n",
    "for _ in range(10):\n",
    "    data_index = [data_index for data_index in range(train_x.shape[0])] # 학습 데이터의 인덱스를 리스트로 변환\n",
    "    random_data_index = np.random.choice(data_index, train_x.shape[0]) # 데이터의 1/10 크기만큼 랜덤 샘플링, // 는 소수점을 무시하기 위함\n",
    "    print(len(set(random_data_index)))\n",
    "    lgb_dtrain = lgb.Dataset(data = train_x.iloc[random_data_index,], label = train_y.iloc[random_data_index,]) # 학습 데이터를 LightGBM 모델에 맞게 변환\n",
    "    lgb_param = {'max_depth': 14, # 트리 깊이\n",
    "            'learning_rate': 0.01, # Step Size\n",
    "            'n_estimators': 500, # Number of trees, 트리 생성 개수\n",
    "            'objective': 'regression'} # 파라미터 추가, Label must be in [0, num_class) -> num_class보다 1 커야한다.\n",
    "    lgb_model = lgb.train(params = lgb_param, train_set = lgb_dtrain) # 학습 진행\n",
    "    predict1 = lgb_model.predict(test_x) # 테스트 데이터 예측\n",
    "    bagging_predict_result.append(predict1) # 반복문이 실행되기 전 빈 리스트에 결과 값 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([522665.88582213, 651946.73946208, 967356.29483743, ...,\n",
       "        350356.57492837, 938071.8083285 , 460615.39964839]),\n",
       " array([498424.24271969, 600269.12948634, 890383.79030474, ...,\n",
       "        356012.3657224 , 964738.72852297, 465301.21906519]),\n",
       " array([515873.29484157, 621807.82226142, 960120.38660572, ...,\n",
       "        347284.38855023, 948646.88193802, 463364.15670236]),\n",
       " array([508506.14195941, 634720.62530188, 911655.78667453, ...,\n",
       "        352192.65267049, 997166.11329281, 476676.6136572 ]),\n",
       " array([519190.3912312 , 645369.55136586, 906795.54437666, ...,\n",
       "        333893.12604488, 912246.92306423, 449089.71387689]),\n",
       " array([514172.716256  , 683095.77619101, 942012.66629715, ...,\n",
       "        320605.00303394, 936972.50410462, 464930.27351948]),\n",
       " array([522439.04456973, 613501.21446924, 892998.55993293, ...,\n",
       "        323079.1103818 , 955497.77340043, 449015.77677654]),\n",
       " array([520963.12362562, 633242.76577141, 961857.31435018, ...,\n",
       "        338049.41275852, 996105.771755  , 472540.59575758]),\n",
       " array([ 505785.51691674,  605905.72539907, 1017396.54084082, ...,\n",
       "         358110.33647349,  886336.80315291,  457337.10139063]),\n",
       " array([516753.00948077, 619053.58372962, 984339.67218417, ...,\n",
       "        333680.01652189, 967062.49223914, 458142.02454403])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagging_predict_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging을 바탕으로 예측한 결과값에 대한 평균을 계산\n",
    "bagging_predict = [] # 빈 리스트 생성\n",
    "for lst2_index in range(test_x.shape[0]): # 테스트 데이터 개수만큼의 반복\n",
    "    temp_predict = [] # 임시 빈 리스트 생성 (반복문 내 결과값 저장)\n",
    "    for lst_index in range(len(bagging_predict_result)): # Bagging 결과 리스트 반복\n",
    "        temp_predict.append(bagging_predict_result[lst_index][lst2_index]) # 각 Bagging 결과 예측한 값 중 같은 인덱스를 리스트에 저장\n",
    "    bagging_predict.append(np.mean(temp_predict)) # 해당 인덱스의 30개의 결과값에 대한 평균을 최종 리스트에 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 210521.34226206047\n"
     ]
    }
   ],
   "source": [
    "# 예측한 결과값들의 평균을 계산하여 실제 테스트 데이트의 타겟변수와 비교하여 성능 평가\n",
    "\n",
    "print(\"RMSE: {}\".format(sqrt(mean_squared_error(bagging_predict, test_y)))) # RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[514477.33674228506,\n",
       " 630891.2933437938,\n",
       " 943491.6556404332,\n",
       " 1677993.918781852,\n",
       " 642898.7743702156,\n",
       " 366576.52694173786,\n",
       " 707675.4511174539,\n",
       " 433727.4002392752,\n",
       " 461044.03354124696,\n",
       " 493495.5996699642,\n",
       " 634886.5269960631,\n",
       " 378631.3934121176,\n",
       " 302052.9864862921,\n",
       " 357738.8962936713,\n",
       " 340390.5258631332,\n",
       " 1295202.1636524373,\n",
       " 369651.474787061,\n",
       " 1002959.6983907601,\n",
       " 319746.4813479397,\n",
       " 525324.5818273493,\n",
       " 377141.9464282272,\n",
       " 1944656.2493357826,\n",
       " 667978.1948595127,\n",
       " 545863.3191408428,\n",
       " 505611.25477348064,\n",
       " 481734.5743063856,\n",
       " 298720.83926935727,\n",
       " 254362.32985993754,\n",
       " 480093.11646613694,\n",
       " 535450.03439459,\n",
       " 478828.2428884682,\n",
       " 474289.16578382987,\n",
       " 469225.42843509524,\n",
       " 569371.3155748271,\n",
       " 382909.2612904702,\n",
       " 1041338.7184109802,\n",
       " 913993.5299666736,\n",
       " 526497.9997446183,\n",
       " 355363.1768507954,\n",
       " 1605503.8114840507,\n",
       " 395583.25355917343,\n",
       " 276275.64082088904,\n",
       " 511223.4396753587,\n",
       " 340928.50415597914,\n",
       " 252793.64610828803,\n",
       " 243779.2271450017,\n",
       " 325819.554817296,\n",
       " 331251.5909195469,\n",
       " 355250.9691788185,\n",
       " 566205.5494337563,\n",
       " 367751.18011935044,\n",
       " 344685.2519191211,\n",
       " 759311.8678073194,\n",
       " 342804.287373179,\n",
       " 462581.5305373748,\n",
       " 1721897.4338919756,\n",
       " 477872.4664612465,\n",
       " 713862.4278860666,\n",
       " 329775.3711819026,\n",
       " 656821.6510650138,\n",
       " 482938.30909291946,\n",
       " 379751.12187139795,\n",
       " 302255.79016819195,\n",
       " 528993.137153504,\n",
       " 442442.9097804222,\n",
       " 283524.90411594126,\n",
       " 383923.05709985207,\n",
       " 1661311.0370286258,\n",
       " 478846.65579229203,\n",
       " 666199.6318529361,\n",
       " 431910.42789854406,\n",
       " 301643.77143300505,\n",
       " 758977.6059415247,\n",
       " 520914.12263776024,\n",
       " 504591.4245502621,\n",
       " 1294151.5563382523,\n",
       " 803979.3798503126,\n",
       " 285672.6286121902,\n",
       " 454607.2531670969,\n",
       " 932163.2896408504,\n",
       " 642788.5619786839,\n",
       " 375307.6681838188,\n",
       " 656809.3418419592,\n",
       " 357936.6031923573,\n",
       " 826561.17098893,\n",
       " 526447.9839964791,\n",
       " 526494.2730537631,\n",
       " 552187.4305053811,\n",
       " 356921.09379429184,\n",
       " 451547.96031327394,\n",
       " 351169.92263058544,\n",
       " 393499.22423890827,\n",
       " 647289.3995673943,\n",
       " 1112040.4459499063,\n",
       " 425673.028115297,\n",
       " 488752.9885018958,\n",
       " 359232.604520471,\n",
       " 309159.6105991484,\n",
       " 817342.4092102818,\n",
       " 457009.41373009933,\n",
       " 256474.35684855678,\n",
       " 923564.8453792995,\n",
       " 1005556.6787569998,\n",
       " 478193.0901453855,\n",
       " 1072742.4074452838,\n",
       " 301034.7897918088,\n",
       " 490009.3949964892,\n",
       " 490209.75126869295,\n",
       " 817871.6657575446,\n",
       " 2381428.363651586,\n",
       " 548708.4875635961,\n",
       " 318779.2814337943,\n",
       " 554487.0602761037,\n",
       " 632728.7470430324,\n",
       " 551406.4742012655,\n",
       " 342798.5241614804,\n",
       " 312779.8224400894,\n",
       " 248318.7331240244,\n",
       " 324189.8892201683,\n",
       " 340390.5258631332,\n",
       " 381242.68420773785,\n",
       " 284314.29250344733,\n",
       " 338990.91882623435,\n",
       " 256523.99799258442,\n",
       " 595919.9140696283,\n",
       " 651521.9555033126,\n",
       " 277363.67759016674,\n",
       " 767919.6860964372,\n",
       " 452061.4425913196,\n",
       " 424033.2628203236,\n",
       " 529626.2677773944,\n",
       " 472486.45541253936,\n",
       " 409788.875770009,\n",
       " 821699.1610360248,\n",
       " 375075.6157824883,\n",
       " 461908.09152221645,\n",
       " 380061.2734344584,\n",
       " 352565.48312697577,\n",
       " 896616.7790377637,\n",
       " 623241.5088560404,\n",
       " 513318.8497262676,\n",
       " 778863.152721674,\n",
       " 915557.6140003651,\n",
       " 401112.8284500255,\n",
       " 256438.12731997384,\n",
       " 382456.09060327976,\n",
       " 487965.2868355101,\n",
       " 244681.74557113898,\n",
       " 413057.0677390882,\n",
       " 473991.9336075045,\n",
       " 581590.5950379525,\n",
       " 683054.0722487088,\n",
       " 555954.2287505018,\n",
       " 1124384.1815858006,\n",
       " 902110.6877604263,\n",
       " 913175.6586893562,\n",
       " 591737.0772213299,\n",
       " 659191.5602195329,\n",
       " 595297.9659281143,\n",
       " 488136.0502050235,\n",
       " 650302.7253275423,\n",
       " 367408.245452504,\n",
       " 331251.5909195469,\n",
       " 356415.0046564775,\n",
       " 364009.0997940075,\n",
       " 347412.9521361086,\n",
       " 298939.7983376818,\n",
       " 312449.93014892784,\n",
       " 448188.29765536607,\n",
       " 462216.91398860066,\n",
       " 630737.5649677786,\n",
       " 396410.6262266802,\n",
       " 462232.81275093724,\n",
       " 576812.0395476392,\n",
       " 420767.04411675397,\n",
       " 412115.31611418223,\n",
       " 352438.023313747,\n",
       " 666389.3535344934,\n",
       " 345053.8416957215,\n",
       " 254381.75925149443,\n",
       " 313124.0619569442,\n",
       " 481451.1574344651,\n",
       " 538033.731366242,\n",
       " 660442.2186922736,\n",
       " 468386.5777988125,\n",
       " 468824.3916559375,\n",
       " 273075.7672351731,\n",
       " 426383.3052405878,\n",
       " 356287.1572122878,\n",
       " 351048.2199777632,\n",
       " 377314.84484001057,\n",
       " 660415.943380501,\n",
       " 1617787.499430605,\n",
       " 1260179.476036123,\n",
       " 262309.56202119956,\n",
       " 481794.58032224455,\n",
       " 497229.39465866843,\n",
       " 1632955.5457343091,\n",
       " 455745.28513806284,\n",
       " 461517.5829085541,\n",
       " 322805.9061470313,\n",
       " 380022.9182682031,\n",
       " 517684.7110413026,\n",
       " 766540.6831814661,\n",
       " 785855.0289229347,\n",
       " 311794.41881326435,\n",
       " 505611.25477348064,\n",
       " 301235.8906824347,\n",
       " 504426.5432665674,\n",
       " 1410986.5677073959,\n",
       " 359232.604520471,\n",
       " 415975.2688165145,\n",
       " 457021.2284687235,\n",
       " 359232.604520471,\n",
       " 330684.41575219354,\n",
       " 723360.4193913675,\n",
       " 793227.7051314849,\n",
       " 347603.9981739097,\n",
       " 363084.3773874289,\n",
       " 357745.46007438464,\n",
       " 1759292.9331227303,\n",
       " 542488.629472831,\n",
       " 499703.6297513971,\n",
       " 450545.33628651325,\n",
       " 533458.9745604829,\n",
       " 755622.4290309389,\n",
       " 342949.42952174565,\n",
       " 1226902.1814837442,\n",
       " 887368.9730461761,\n",
       " 460720.6832016186,\n",
       " 356405.45154342626,\n",
       " 484957.90019644576,\n",
       " 722504.7789073574,\n",
       " 301114.6121082486,\n",
       " 344571.7078419191,\n",
       " 385633.0285370529,\n",
       " 351267.9423981095,\n",
       " 351281.3976940233,\n",
       " 2272642.0242562937,\n",
       " 344927.79307746084,\n",
       " 442632.81894649164,\n",
       " 459793.7931291773,\n",
       " 605656.8280633629,\n",
       " 419005.31862779486,\n",
       " 468049.1670539896,\n",
       " 309418.1526552192,\n",
       " 521927.9235817526,\n",
       " 553665.4587782812,\n",
       " 705219.9362517828,\n",
       " 860592.4218410648,\n",
       " 538634.1955698091,\n",
       " 435660.2170352365,\n",
       " 726779.610697341,\n",
       " 350480.9523769503,\n",
       " 351048.2199777632,\n",
       " 527906.8908108138,\n",
       " 502203.75369513984,\n",
       " 471237.9290959934,\n",
       " 889777.1793466514,\n",
       " 368861.53708432673,\n",
       " 3198190.737520081,\n",
       " 639157.9679289979,\n",
       " 759375.6569229115,\n",
       " 1107003.0546014025,\n",
       " 502334.2597092756,\n",
       " 677062.3990322187,\n",
       " 911456.7184435427,\n",
       " 350761.29276479466,\n",
       " 712875.9857394117,\n",
       " 435077.51690574334,\n",
       " 472309.9592027771,\n",
       " 341991.47065177653,\n",
       " 288276.43606105365,\n",
       " 435032.3697719807,\n",
       " 408473.06228141056,\n",
       " 1373592.9916867127,\n",
       " 306885.92858639976,\n",
       " 344202.24734253425,\n",
       " 525399.8266022612,\n",
       " 362427.5898501384,\n",
       " 313327.77818664105,\n",
       " 505387.55190004205,\n",
       " 367027.00002443383,\n",
       " 443485.3432429627,\n",
       " 486342.8709022724,\n",
       " 448516.9327753622,\n",
       " 369014.8408932069,\n",
       " 642424.0896738262,\n",
       " 353734.818573892,\n",
       " 316303.50063230016,\n",
       " 804260.771116073,\n",
       " 444388.01656752673,\n",
       " 257588.47803452372,\n",
       " 348820.9835388871,\n",
       " 660415.943380501,\n",
       " 657680.0067485293,\n",
       " 486659.6193936247,\n",
       " 449612.9975127863,\n",
       " 455727.9319964299,\n",
       " 579652.6790210286,\n",
       " 471944.9201737136,\n",
       " 555641.1339726489,\n",
       " 328574.7166249471,\n",
       " 574320.5322892141,\n",
       " 342573.7605157734,\n",
       " 797591.3593359214,\n",
       " 448188.29765536607,\n",
       " 387089.2008730608,\n",
       " 331644.3591823152,\n",
       " 328091.92249061394,\n",
       " 365222.72516641475,\n",
       " 290120.7176585692,\n",
       " 871661.8129791704,\n",
       " 1593489.1401965762,\n",
       " 974699.9624001116,\n",
       " 444966.56884231727,\n",
       " 815534.737691097,\n",
       " 453887.4264662409,\n",
       " 790618.1483979841,\n",
       " 345443.6836278737,\n",
       " 398569.2970272384,\n",
       " 496204.3572932806,\n",
       " 262309.56202119956,\n",
       " 302803.9657690039,\n",
       " 460706.0099500575,\n",
       " 462216.91398860066,\n",
       " 554149.870981517,\n",
       " 301787.30928953463,\n",
       " 506169.10144007904,\n",
       " 256523.99799258442,\n",
       " 658574.6732963992,\n",
       " 297555.11320112384,\n",
       " 500828.8999650421,\n",
       " 303379.901898854,\n",
       " 387362.3010412558,\n",
       " 466515.3853452137,\n",
       " 602281.8115698828,\n",
       " 478716.84066224715,\n",
       " 942023.1446048204,\n",
       " 259959.47654373798,\n",
       " 1675792.3958516626,\n",
       " 475245.494599952,\n",
       " 438951.1435072582,\n",
       " 560092.425882077,\n",
       " 687347.2086752837,\n",
       " 623720.6580141948,\n",
       " 342216.5006962436,\n",
       " 459752.8129262167,\n",
       " 476596.7373247718,\n",
       " 735464.7318768952,\n",
       " 282634.8708973903,\n",
       " 363984.2091383944,\n",
       " 475537.03848653025,\n",
       " 629924.6778589729,\n",
       " 337736.04688549315,\n",
       " 845426.7516670708,\n",
       " 554397.2831658912,\n",
       " 921136.7998162757,\n",
       " 996208.5406085455,\n",
       " 643169.2105652806,\n",
       " 369238.59664536756,\n",
       " 793944.9221322576,\n",
       " 356470.8113820598,\n",
       " 562961.6519697214,\n",
       " 675407.0613038068,\n",
       " 327666.30557248846,\n",
       " 762354.2314680341,\n",
       " 395375.6524148915,\n",
       " 1002829.70576769,\n",
       " 386278.42416600825,\n",
       " 500580.755481675,\n",
       " 682323.083858181,\n",
       " 586336.0135486572,\n",
       " 353391.54860910465,\n",
       " 544215.29387302,\n",
       " 381777.03655808116,\n",
       " 335089.79641018563,\n",
       " 551255.901500551,\n",
       " 386687.2586443269,\n",
       " 415773.46732205123,\n",
       " 372061.1707841127,\n",
       " 708088.1926026066,\n",
       " 644520.3762143884,\n",
       " 442212.186860618,\n",
       " 460212.4429907738,\n",
       " 461004.84703764756,\n",
       " 302950.200214367,\n",
       " 489580.97843585594,\n",
       " 425766.3273092039,\n",
       " 462117.00191447296,\n",
       " 565355.0783732745,\n",
       " 391361.5160446201,\n",
       " 376731.74997570156,\n",
       " 410201.3539398903,\n",
       " 1429736.7877158984,\n",
       " 386278.42416600825,\n",
       " 345291.65289425565,\n",
       " 1234622.4544185498,\n",
       " 671943.450211221,\n",
       " 385793.6561975436,\n",
       " 431910.42789854406,\n",
       " 475867.1053400566,\n",
       " 649469.0531634645,\n",
       " 455879.72620005516,\n",
       " 392307.2966241514,\n",
       " 438316.9573999335,\n",
       " 462216.91398860066,\n",
       " 424033.2628203236,\n",
       " 482378.6023199825,\n",
       " 468908.9593506991,\n",
       " 351068.43928715395,\n",
       " 474425.19921424426,\n",
       " 610039.1534528586,\n",
       " 418729.46568265645,\n",
       " 265619.7262466765,\n",
       " 383625.1538579107,\n",
       " 461831.3559208544,\n",
       " 449422.9837516472,\n",
       " 1129098.8769148593,\n",
       " 455855.59575234633,\n",
       " 397121.48622017365,\n",
       " 558222.0026502023,\n",
       " 331457.672192532,\n",
       " 680116.0271463639,\n",
       " 402793.9277302051,\n",
       " 476791.94586971664,\n",
       " 478686.86164018407,\n",
       " 467098.5833036024,\n",
       " 557387.9727823107,\n",
       " 333153.85819967545,\n",
       " 610339.1446292881,\n",
       " 511443.95272677595,\n",
       " 703816.0639464932,\n",
       " 533634.3493851835,\n",
       " 513883.7327636147,\n",
       " 474204.7142372844,\n",
       " 525016.7460051476,\n",
       " 381730.90361262753,\n",
       " 362851.46884984145,\n",
       " 351787.6978688424,\n",
       " 658097.5562996606,\n",
       " 383114.030086096,\n",
       " 360127.72316103254,\n",
       " 272746.2306064175,\n",
       " 385051.83769114944,\n",
       " 384502.5972679607,\n",
       " 821699.1610360248,\n",
       " 2363964.956096465,\n",
       " 447247.9927933947,\n",
       " 1177110.22665407,\n",
       " 500478.42801702244,\n",
       " 264301.00666497345,\n",
       " 486667.0949054315,\n",
       " 448204.8104175384,\n",
       " 431849.8326241064,\n",
       " 684766.5262564538,\n",
       " 380169.15652066574,\n",
       " 817665.1407736005,\n",
       " 372244.3309996713,\n",
       " 277577.51993038214,\n",
       " 468490.9838508152,\n",
       " 644092.9927932653,\n",
       " 1009365.787637466,\n",
       " 374585.2916827975,\n",
       " 715940.9042571726,\n",
       " 450674.8106516472,\n",
       " 354889.7074668459,\n",
       " 541791.687201672,\n",
       " 960096.104342922,\n",
       " 431271.6321933321,\n",
       " 478716.84066224715,\n",
       " 351068.43928715395,\n",
       " 455897.1507744639,\n",
       " 1167522.7408701866,\n",
       " 444966.66818712384,\n",
       " 490739.3124517564,\n",
       " 386900.40531265276,\n",
       " 454001.39852264867,\n",
       " 319918.34153921215,\n",
       " 453674.5194052538,\n",
       " 304604.3048234327,\n",
       " 334819.1820346986,\n",
       " 347244.6481978585,\n",
       " 384565.2415538065,\n",
       " 435713.134777633,\n",
       " 629741.7845175974,\n",
       " 527635.3652930257,\n",
       " 334772.73799405404,\n",
       " 769877.6139581272,\n",
       " 648321.1952959264,\n",
       " 683317.9526334639,\n",
       " 348927.66677732335,\n",
       " 420767.04411675397,\n",
       " 662105.4860600977,\n",
       " 1835149.1598580047,\n",
       " 494634.42217875505,\n",
       " 680690.9752751708,\n",
       " 367059.9335682202,\n",
       " 372837.9189859535,\n",
       " 351625.70595100295,\n",
       " 696482.9226138361,\n",
       " 482938.30909291946,\n",
       " 319370.0117656389,\n",
       " 520048.1025489604,\n",
       " 349677.4069677318,\n",
       " 553191.3609847056,\n",
       " 801949.6162849087,\n",
       " 1009875.6136618467,\n",
       " 329466.27888440015,\n",
       " 535879.7090691362,\n",
       " 1212494.1315902588,\n",
       " 462216.91398860066,\n",
       " 815405.9123304336,\n",
       " 631592.3425101626,\n",
       " 461692.0335766103,\n",
       " 826975.7914217721,\n",
       " 719825.4252081176,\n",
       " 252702.91139668948,\n",
       " 379751.12187139795,\n",
       " 282509.8737930359,\n",
       " 429542.43074268085,\n",
       " 532176.6261595625,\n",
       " 334416.3699898723,\n",
       " 500234.9718036443,\n",
       " 537475.758427548,\n",
       " 406803.1525968572,\n",
       " 494464.7599949891,\n",
       " 356365.897156466,\n",
       " 825191.0973226625,\n",
       " 341360.693691954,\n",
       " 373925.0097423358,\n",
       " 364970.5118923565,\n",
       " 1218275.1288475883,\n",
       " 460007.90468586085,\n",
       " 297455.1934087859,\n",
       " 283136.7841896039,\n",
       " 459424.5623698748,\n",
       " 806128.5938869474,\n",
       " 474136.1362617804,\n",
       " 672570.3872348408,\n",
       " 542698.5066043723,\n",
       " 450769.4697111788,\n",
       " 1020065.3576097495,\n",
       " 280158.9147008981,\n",
       " 910165.3636359919,\n",
       " 644096.8938662505,\n",
       " 516393.7620398877,\n",
       " 1049417.37958705,\n",
       " 462569.55933351704,\n",
       " 287100.7370055413,\n",
       " 833025.8044533891,\n",
       " 341226.93853008526,\n",
       " 636588.2760892622,\n",
       " 1425308.0530360686,\n",
       " 346172.34054394014,\n",
       " 501418.2387571765,\n",
       " 448000.73820242117,\n",
       " 366095.06448160886,\n",
       " 252809.30184042506,\n",
       " 877929.7741937786,\n",
       " 434591.20465207455,\n",
       " 297570.22489734105,\n",
       " 472545.2900993164,\n",
       " 353998.6529606382,\n",
       " 296731.30109806004,\n",
       " 490572.1961845936,\n",
       " 355763.30485068954,\n",
       " 414457.50232842425,\n",
       " 393133.8752847208,\n",
       " 483504.71246960294,\n",
       " 481595.1788446826,\n",
       " 533664.5704429207,\n",
       " 778460.0263593319,\n",
       " 331573.44945387705,\n",
       " 331998.86847713,\n",
       " 383114.030086096,\n",
       " 380022.9182682031,\n",
       " 332361.31567499245,\n",
       " 405111.35273747426,\n",
       " 768117.1573971353,\n",
       " 500243.33673679654,\n",
       " 859568.878277502,\n",
       " 841032.6601742381,\n",
       " 476497.97523889196,\n",
       " 443629.44669262937,\n",
       " 479928.5363400021,\n",
       " 315871.98088215647,\n",
       " 402770.40493234154,\n",
       " 440540.9347572336,\n",
       " 322092.3896712912,\n",
       " 474084.9190602529,\n",
       " 672570.3872348408,\n",
       " 418874.8587914581,\n",
       " 406317.9106542148,\n",
       " 463759.5767326212,\n",
       " 408433.756357903,\n",
       " 1009266.912125566,\n",
       " 758889.6716405046,\n",
       " 760632.030658087,\n",
       " 370919.68974982505,\n",
       " 461004.84703764756,\n",
       " 403449.5295128912,\n",
       " 564480.8321521583,\n",
       " 262057.571612457,\n",
       " 389801.6995472127,\n",
       " 350944.9256123287,\n",
       " 315805.11527423904,\n",
       " 355980.0145721817,\n",
       " 283573.5179244626,\n",
       " 413980.7412671062,\n",
       " 589596.0038424757,\n",
       " 310358.7499958279,\n",
       " 440629.98245307617,\n",
       " 512341.8548923499,\n",
       " 489876.65986150585,\n",
       " 360034.5015438335,\n",
       " 334867.3065503111,\n",
       " 283646.10403033893,\n",
       " 531683.6969853415,\n",
       " 470693.7625731031,\n",
       " 1713979.0827501244,\n",
       " 461692.0335766103,\n",
       " 2133584.71107719,\n",
       " 678148.9638274636,\n",
       " 1043952.6275973127,\n",
       " 602206.647340461,\n",
       " 412667.14336334413,\n",
       " 453402.63345721504,\n",
       " 983749.9930823169,\n",
       " 700893.1839712339,\n",
       " 469705.7697106421,\n",
       " 477786.3446786185,\n",
       " 740099.1557147161,\n",
       " 492890.17663301155,\n",
       " 386393.26271085604,\n",
       " 399900.7605411337,\n",
       " 363171.9371367775,\n",
       " 468459.22204108007,\n",
       " 335049.1297683888,\n",
       " 724328.1353143542,\n",
       " 504716.13918462145,\n",
       " 793944.9221322576,\n",
       " 691817.556575826,\n",
       " 504450.4856422258,\n",
       " 447571.05473964615,\n",
       " 482036.47780888696,\n",
       " 487052.7523853384,\n",
       " 503037.8837505481,\n",
       " 569322.5080303539,\n",
       " 309270.2147049421,\n",
       " 1066217.244148569,\n",
       " 312120.71634241124,\n",
       " 582350.9833296898,\n",
       " 277371.56276617,\n",
       " 1777192.7648820665,\n",
       " 838587.6196448308,\n",
       " 628461.562683464,\n",
       " 504347.6547752274,\n",
       " 653246.0060322613,\n",
       " 571140.4565782612,\n",
       " 349794.9647448672,\n",
       " 528680.0469660349,\n",
       " 460491.76919336774,\n",
       " 1023909.7488594118,\n",
       " 376417.65936700604,\n",
       " 493507.44653705496,\n",
       " 1007536.7453470489,\n",
       " 529906.3047994624,\n",
       " 453365.21178147383,\n",
       " 448893.5003228955,\n",
       " 1078302.7885268615,\n",
       " 320391.5010519174,\n",
       " 638726.6451047672,\n",
       " 785839.4078071051,\n",
       " 320391.5010519174,\n",
       " 631514.7707715554,\n",
       " 603404.2612748477,\n",
       " 397323.3340292146,\n",
       " 474427.30088030547,\n",
       " 623462.6767273917,\n",
       " 429939.7083340511,\n",
       " 348927.66677732335,\n",
       " 534051.6456345543,\n",
       " 277371.56276617,\n",
       " 347863.23925925326,\n",
       " 469698.94684591657,\n",
       " 538317.1044691878,\n",
       " 441361.7209510001,\n",
       " 808324.0996087834,\n",
       " 412173.88710906246,\n",
       " 302052.9864862921,\n",
       " 323652.6487431765,\n",
       " 468908.9593506991,\n",
       " 731734.1479100125,\n",
       " 773060.7918852888,\n",
       " 368861.53708432673,\n",
       " 314388.80761781253,\n",
       " 383105.8058515043,\n",
       " 271166.29477576684,\n",
       " 855488.434442799,\n",
       " 401025.0625857937,\n",
       " 284451.0665095881,\n",
       " 337845.56381237565,\n",
       " 490967.5975522756,\n",
       " 492084.4718596825,\n",
       " 468908.9593506991,\n",
       " 1137161.082691314,\n",
       " 1076612.3081259693,\n",
       " 1181751.5365618584,\n",
       " 435085.94854244526,\n",
       " 590965.790281341,\n",
       " 364523.07819919544,\n",
       " 527792.8036532004,\n",
       " 356767.38789303106,\n",
       " 390488.1144217819,\n",
       " 344484.7986402301,\n",
       " 816451.5478550717,\n",
       " 462581.5305373748,\n",
       " 349702.53201148333,\n",
       " 394017.7841270141,\n",
       " 294086.01023467013,\n",
       " 540421.7286140976,\n",
       " 484288.84237783914,\n",
       " 420911.05654688814,\n",
       " 521046.92729185976,\n",
       " 283464.85007010715,\n",
       " 639060.976611147,\n",
       " 521336.5104410981,\n",
       " 454607.2531670969,\n",
       " 471944.9201737136,\n",
       " 300883.89018433174,\n",
       " 359609.6640815119,\n",
       " 1161720.8336992273,\n",
       " 755587.4362567348,\n",
       " 443464.7373231395,\n",
       " 321335.4540509612,\n",
       " 352832.7260003192,\n",
       " 1036355.476153017,\n",
       " 554054.1784899343,\n",
       " 427254.3047237688,\n",
       " 277504.93382450595,\n",
       " 378414.3348968252,\n",
       " 293727.2942700903,\n",
       " 873606.4351940025,\n",
       " 252101.93066770947,\n",
       " 324119.2147992285,\n",
       " 298720.83926935727,\n",
       " 468049.1670539896,\n",
       " 479999.9841164279,\n",
       " 589189.7653979299,\n",
       " 576084.3035678986,\n",
       " 386183.1614197433,\n",
       " 468102.66611317464,\n",
       " 562006.6492725031,\n",
       " 478846.65579229203,\n",
       " 470576.94540885964,\n",
       " 623241.5088560404,\n",
       " 506089.64040835935,\n",
       " 779038.5911072285,\n",
       " 388634.826552744,\n",
       " 317421.68406966294,\n",
       " 465376.3442942871,\n",
       " 586931.170413164,\n",
       " 474954.02453148377,\n",
       " 491856.21717769,\n",
       " 947574.653499516,\n",
       " 534305.1354390349,\n",
       " 755437.5503994401,\n",
       " 626165.83793813,\n",
       " 836414.6469987702,\n",
       " 277548.0674020007,\n",
       " 469418.7103219678,\n",
       " 466202.17026490055,\n",
       " 814803.5399635502,\n",
       " 860246.9061348578,\n",
       " 277223.38049522263,\n",
       " 256438.12731997384,\n",
       " 471829.6283589581,\n",
       " 296739.28053131484,\n",
       " 725468.0708295463,\n",
       " 388865.54929349956,\n",
       " 475824.78696295654,\n",
       " 511267.4130165604,\n",
       " 381264.1048270025,\n",
       " 298720.83926935727,\n",
       " 292833.1105033757,\n",
       " 336477.9441320798,\n",
       " 635747.9648975995,\n",
       " 282623.1909412241,\n",
       " 371052.2493107623,\n",
       " 477035.156388438,\n",
       " 391428.993162411,\n",
       " 525016.7460051476,\n",
       " 372758.3540054555,\n",
       " 390488.1144217819,\n",
       " 462581.5305373748,\n",
       " 355877.285304293,\n",
       " 1310124.266419536,\n",
       " 609864.4260929435,\n",
       " 337217.0846498258,\n",
       " 454053.6015954752,\n",
       " 745785.1090596863,\n",
       " 641687.888287041,\n",
       " 274951.4532740934,\n",
       " 469481.0441774131,\n",
       " 387362.3010412558,\n",
       " 301202.14781041926,\n",
       " 857834.9580082992,\n",
       " 475515.00314920256,\n",
       " 550036.8271500769,\n",
       " 479464.9328279553,\n",
       " 688379.3736176897,\n",
       " 582911.4557212975,\n",
       " 869041.0777307742,\n",
       " 666199.6318529361,\n",
       " 802666.92399976,\n",
       " 381187.0828373154,\n",
       " 985835.4370790205,\n",
       " 616745.6969577768,\n",
       " 787137.5903932399,\n",
       " 457183.52147387835,\n",
       " 472464.91175658023,\n",
       " 380169.15652066574,\n",
       " 462581.5305373748,\n",
       " 379751.12187139795,\n",
       " 778863.152721674,\n",
       " 526436.5702598431,\n",
       " 482049.2391516104,\n",
       " 279038.38605879934,\n",
       " 277371.56276617,\n",
       " 302341.5411789281,\n",
       " 818848.8290447335,\n",
       " 656821.6510650138,\n",
       " 460201.9983099712,\n",
       " 372158.1101073298,\n",
       " 447817.6431565186,\n",
       " 365222.72516641475,\n",
       " 657442.4863313901,\n",
       " 277320.54642246576,\n",
       " 650996.1434754676,\n",
       " 520169.28450850246,\n",
       " 352668.68673925765,\n",
       " 278506.9133698509,\n",
       " 478846.65579229203,\n",
       " 501338.92115989496,\n",
       " 376177.4107238025,\n",
       " 683888.1714275755,\n",
       " 334819.1820346986,\n",
       " 388865.54929349956,\n",
       " 546248.4721421765,\n",
       " 304607.7252814601,\n",
       " 577747.385008621,\n",
       " 683225.7886853444,\n",
       " 699706.6546057592,\n",
       " 483954.47399788257,\n",
       " 422624.96051610156,\n",
       " 712881.2758667588,\n",
       " 514309.58782417467,\n",
       " 656821.6510650138,\n",
       " 1491335.2174760841,\n",
       " 531705.4403736326,\n",
       " 347575.21974919864,\n",
       " 688118.4471628309,\n",
       " 822894.2398750256,\n",
       " 340742.7259428819,\n",
       " 405179.47611219244,\n",
       " 375261.78162758006,\n",
       " 290086.7541910216,\n",
       " 509717.12924481416,\n",
       " 499841.04204267013,\n",
       " 411068.36411641317,\n",
       " 490360.8789828019,\n",
       " 340550.02231315686,\n",
       " 371432.02542169613,\n",
       " 252678.91794622494,\n",
       " 352565.48312697577,\n",
       " 345291.65289425565,\n",
       " 329109.6083367793,\n",
       " 343275.98692197143,\n",
       " 331527.5349735659,\n",
       " 827281.9440765431,\n",
       " 418030.3906690207,\n",
       " 433564.28854724485,\n",
       " 707280.4888270751,\n",
       " 354091.6306832639,\n",
       " 252101.93066770947,\n",
       " 369215.84294980066,\n",
       " 291361.46211381163,\n",
       " 583376.2700282497,\n",
       " 302533.2242216326,\n",
       " 513484.0103699006,\n",
       " 808986.5590935583,\n",
       " 572119.9337419472,\n",
       " 564523.7422376822,\n",
       " 501957.13897756394,\n",
       " 269434.89272860915,\n",
       " 957427.0858960601,\n",
       " 642898.7743702156,\n",
       " 860592.4218410648,\n",
       " 327052.93579681095,\n",
       " 461746.510255081,\n",
       " 802879.2811669286,\n",
       " 511464.7947487803,\n",
       " 254440.67917577474,\n",
       " 1098723.5245084367,\n",
       " 313125.36994246626,\n",
       " 642191.3282014735,\n",
       " 472545.2900993164,\n",
       " 786366.1013706718,\n",
       " 549688.0789087436,\n",
       " 450674.8106516472,\n",
       " 461004.84703764756,\n",
       " 442493.57420298905,\n",
       " 560661.5934632217,\n",
       " 1259283.1118255458,\n",
       " 461870.01969077194,\n",
       " 825040.6883602019,\n",
       " 488049.30338496657,\n",
       " 384332.9766789969,\n",
       " 275833.4311854295,\n",
       " 426578.50769905857,\n",
       " 561383.4093969243,\n",
       " 485612.30113391083,\n",
       " 452518.41339888994,\n",
       " 293727.2942700903,\n",
       " 362988.7769212189,\n",
       " 351390.51824815973,\n",
       " 504261.613390116,\n",
       " 562556.5705411242,\n",
       " 283524.90411594126,\n",
       " 379174.00137193,\n",
       " 502399.6327234368,\n",
       " 631514.7707715554,\n",
       " 999738.8178533136,\n",
       " 666504.4826127433,\n",
       " 360552.7866785644,\n",
       " 781240.0909529133,\n",
       " 718000.764325789,\n",
       " 837470.7151390745,\n",
       " 353688.5774394923,\n",
       " 2205454.4595503286,\n",
       " 368249.9899508025,\n",
       " 382061.3415157334,\n",
       " 501819.57855526486,\n",
       " 331205.14687890233,\n",
       " 353155.3770081503,\n",
       " 556148.0099425085,\n",
       " 663062.5908919638,\n",
       " 695802.767301182,\n",
       " 277177.221705946,\n",
       " 353391.54860910465,\n",
       " 554403.2656586578,\n",
       " 2346525.2522327234,\n",
       " 552511.0386291842,\n",
       " 464983.07758197264,\n",
       " 914580.0251795534,\n",
       " 618164.1899607839,\n",
       " 557313.2661167004,\n",
       " 804339.8493375822,\n",
       " 719767.4460564988,\n",
       " 450674.8106516472,\n",
       " 296514.5510812643,\n",
       " 430559.6863997433,\n",
       " 678318.5696463664,\n",
       " 553967.1171915459,\n",
       " 282807.7758760834,\n",
       " 301134.5888284693,\n",
       " 565801.6760833168,\n",
       " 409988.8116425803,\n",
       " 686084.0983668172,\n",
       " 492805.01299032645,\n",
       " 920694.1304032974,\n",
       " 377362.6508210404,\n",
       " 291036.4365564343,\n",
       " 393022.6062892978,\n",
       " 286317.7657085262,\n",
       " 359232.604520471,\n",
       " 340742.7259428819,\n",
       " 396685.1665735609,\n",
       " 901821.8289661014,\n",
       " 595327.990366186,\n",
       " 364198.7762008178,\n",
       " 510576.5744439812,\n",
       " 667123.171676174,\n",
       " 255317.37446153065,\n",
       " 529474.5851087073,\n",
       " 364959.8304458738,\n",
       " 650974.0444173666,\n",
       " 656747.9351214285,\n",
       " 459998.8707222513,\n",
       " 286694.77323643863,\n",
       " 435502.5782941122,\n",
       " 350931.586355246,\n",
       " 869432.128070465,\n",
       " 358886.8076690783,\n",
       " 555396.5648179216,\n",
       " 413458.92747365485,\n",
       " 386900.40531265276,\n",
       " 355032.9737811418,\n",
       " 344101.06593960884,\n",
       " ...]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagging_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
